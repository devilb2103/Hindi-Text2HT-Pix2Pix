{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9665244,"sourceType":"datasetVersion","datasetId":5872887},{"sourceId":9800448,"sourceType":"datasetVersion","datasetId":6006399},{"sourceId":9800478,"sourceType":"datasetVersion","datasetId":6006423},{"sourceId":155564,"sourceType":"modelInstanceVersion","modelInstanceId":132192,"modelId":154990}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom PIL import Image\nimport albumentations as A\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import save_image\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\n\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pre-requisite Imports","metadata":{}},{"cell_type":"markdown","source":"#### Dataloader transforms","metadata":{}},{"cell_type":"code","source":"both_transform = A.Compose(\n    [A.Resize(width=256, height=256),], additional_targets={\"image0\": \"image\"},\n)\n\ntransform_only_input = A.Compose(\n    [\n#         A.HorizontalFlip(p=0.0),\n        A.ColorJitter(p=0.2),\n        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n        ToTensorV2(),\n    ]\n)\n\ntransform_only_mask = A.Compose(\n    [\n        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n        ToTensorV2(),\n    ]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Dataloader","metadata":{}},{"cell_type":"code","source":"class WordDataset(Dataset):\n    def __init__(self, root_dir):\n        print(os.getcwd())\n        # self.root_dir = os.path.normpath(os.path.join(os.getcwd(), root_dir))\n        self.root_dir = root_dir\n        # self.list_files = os.listdir(self.root_dir)\n        self.list_files = [f'/kaggle/input/typed-and-handwritten-hindi-text/ConcatenatedImages/{i}.png' for i in range(20000, 100000)]\n        print(f\"{len(self.list_files)} files loaded\")\n\n    def __len__(self):\n        return len(self.list_files)\n\n    def __getitem__(self, index):\n        img_file = self.list_files[index]\n        img_path = os.path.join(self.root_dir, img_file)\n        image = np.array(Image.open(img_path))\n        # input_image = image[:, :256, :]\n        # target_image = image[:, 256:, :]\n        input_image = image[:, :256]\n        target_image = image[:, 256:]\n\n        augmentations = both_transform(image=input_image, image0=target_image)\n        input_image = augmentations[\"image\"]\n        target_image = augmentations[\"image0\"]\n\n        input_image = transform_only_input(image=input_image)[\"image\"]\n        target_image = transform_only_mask(image=target_image)[\"image\"]\n\n        return input_image, target_image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### generator","metadata":{}},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, in_channels, out_channels, down=True, act='relu', use_dropout=False):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False, padding_mode=\"reflect\")\n            if down\n            else nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU() if act=='relu' else nn.LeakyReLU(0.2),\n        )\n        self.use_dropout = use_dropout\n        self.dropout = nn.Dropout(0.5)\n        self.down = down\n    def forward(self, x):\n        x = self.conv(x)\n        return self.dropout(x) if self.use_dropout else x\n\nclass Generator(nn.Module):\n    def __init__(self, in_channels=3, features=64):\n        super().__init__()\n        self.initial_down = nn.Sequential(\n            nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n            nn.LeakyReLU(0.2),\n        )\n\n        self.down1 = Block(features, features*2, down=True, act='leaky', use_dropout=False)\n        self.down2 = Block(features*2, features*4, down=True, act='leaky', use_dropout=False) # 32\n        self.down3 = Block(features*4, features*8, down=True, act='leaky', use_dropout=False) # 16\n        self.down4 = Block(features*8, features*8, down=True, act='leaky', use_dropout=False) # 8\n        self.down5 = Block(features*8, features*8, down=True, act='leaky', use_dropout=False) # 4\n        self.down6 = Block(features*8, features*8, down=True, act='leaky', use_dropout=False) # 2\n\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(features*8, features*8, 4, 2, 1), # 1 x 1\n            nn.ReLU(),\n        )\n\n        self.up1 = Block(features*8, features*8, down=False, act='relu', use_dropout=True)\n        self.up2 = Block(features*8*2, features*8, down=False, act='relu', use_dropout=True)\n        self.up3 = Block(features*8*2, features*8, down=False, act='relu', use_dropout=True)\n        self.up4 = Block(features*8*2, features*8, down=False, act='relu', use_dropout=False)\n        self.up5 = Block(features*8*2, features*4, down=False, act='relu', use_dropout=False)\n        self.up6 = Block(features*4*2, features*2, down=False, act='relu', use_dropout=False)\n        self.up7 = Block(features*2*2, features, down=False, act='relu', use_dropout=False)\n\n        self.final_up = nn.Sequential(\n            nn.ConvTranspose2d(features*2, in_channels, kernel_size=4, stride=2, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        d1 = self.initial_down(x)\n        d2 = self.down1(d1)\n        d3 = self.down2(d2)\n        d4 = self.down3(d3)\n        d5 = self.down4(d4)\n        d6 = self.down5(d5)\n        d7 = self.down6(d6)\n\n        bottleneck = self.bottleneck(d7)\n\n        up1 = self.up1(bottleneck)\n        up2 = self.up2(torch.cat([up1, d7], 1))\n        up3 = self.up3(torch.cat([up2, d6], 1))\n        up4 = self.up4(torch.cat([up3, d5], 1))\n        up5 = self.up5(torch.cat([up4, d4], 1))\n        up6 = self.up6(torch.cat([up5, d3], 1))\n        up7 = self.up7(torch.cat([up6, d2], 1))\n\n        return self.final_up(torch.cat([up7, d1], 1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Utils","metadata":{}},{"cell_type":"code","source":"def save_some_examples(gen, val_loader, epoch, folder):\n    x, y = next(iter(val_loader))\n    x, y = x.to(DEVICE), y.to(DEVICE)\n    gen.eval()\n    with torch.no_grad():\n        y_fake = gen(x)\n        y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n        save_image(x * 0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n        if epoch == 1:\n            save_image(y * 0.5 + 0.5, folder + f\"/label_{epoch}.png\")\n    gen.train()\n\n\ndef save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, filename)\n\n\ndef load_checkpoint(checkpoint_file, model, optimizer, lr):\n    print(\"=> Loading checkpoint\")\n    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n    \n    state_dict = checkpoint[\"state_dict\"]\n    \n    # If the keys are prefixed with \"module.\", remove it\n    new_state_dict = {}\n    for k, v in state_dict.items():\n        if k.startswith(\"module.\"):\n            new_state_dict[k[7:]] = v  # Remove the \"module.\" part\n        else:\n            new_state_dict[k] = v\n\n    # print(new_state_dict.keys())\n    model.load_state_dict(new_state_dict)\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    # If we don't do this then it will just have learning rate of old checkpoint\n    # and it will lead to many hours of debugging \\:\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Results from validation set","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nVAL_DIR = \"/kaggle/input/typed-and-handwritten-hindi-text/ConcatenatedImages\"\nMODEL_DIR = \"/kaggle/input/hindi-text2ht-pix2pix-generator/pytorch/epoch-58/1/gen.pth.tar\"\n\nLEARNING_RATE = 2e-4\n\ngen = Generator(in_channels=1, features=64).to(DEVICE)\ngen.eval()\nopt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\nload_checkpoint(os.path.normpath(os.path.join(os.getcwd(), MODEL_DIR)), gen, opt_gen, LEARNING_RATE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_dataset = WordDataset(root_dir=os.path.normpath(os.path.join(os.getcwd(), VAL_DIR)))\nval_loader = DataLoader(val_dataset, batch_size=100, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nog_dir, gen_dir, tgt_dir = \"/kaggle/working/original\", \"/kaggle/working/generated\", \"/kaggle/working/target\"\n\nos.makedirs(og_dir, exist_ok=True)\nos.makedirs(gen_dir, exist_ok=True)\nos.makedirs(tgt_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T09:40:48.616094Z","iopub.execute_input":"2024-11-04T09:40:48.616505Z","iopub.status.idle":"2024-11-04T09:40:48.622812Z","shell.execute_reply.started":"2024-11-04T09:40:48.616463Z","shell.execute_reply":"2024-11-04T09:40:48.621482Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"loop = tqdm(val_loader, leave=True)\n\nbatch_size = 100\n\nfor idx, (x, y) in enumerate(loop):\n    x = x.to(DEVICE)\n    y = y.to(DEVICE)\n\n    # Train Discriminator\n    with torch.amp.autocast('cuda'):\n        y_fake = gen(x)\n        for i in range(batch_size):\n            og_img = x[i].detach().cpu().numpy()[0]\n            gen_img = y_fake[i].detach().cpu().numpy()[0]\n            tgt_img = y[i].detach().cpu().numpy()[0]\n\n            plt.imsave(f'/kaggle/working/original/{idx*batch_size + i + 1}.png', og_img, cmap='gray')\n            plt.imsave(f'/kaggle/working/generated/{idx*batch_size + i + 1}.png', gen_img, cmap='gray')\n            plt.imsave(f'/kaggle/working/target/{idx*batch_size + i + 1}.png', tgt_img, cmap='gray')\n            \n            # plt.figure(figsize=(9, 3))\n            # plt.subplot(1, 3, 1)\n            # plt.imshow(og_img, cmap='gray')\n            # plt.title(f'Input {i}')\n            # plt.subplot(1, 3, 2)\n            # plt.imshow(gen_img, cmap='gray')\n            # plt.title(f'Generated {i}')\n            # plt.subplot(1, 3, 3)\n            # plt.imshow(tgt_img, cmap='gray')\n            # plt.title(f'Label {i}')\n            # plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate HTR Results for all dirs","metadata":{}},{"cell_type":"code","source":"!pip install editdistance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T09:39:58.518379Z","iopub.execute_input":"2024-11-04T09:39:58.518802Z","iopub.status.idle":"2024-11-04T09:40:10.067801Z","shell.execute_reply.started":"2024-11-04T09:39:58.518756Z","shell.execute_reply":"2024-11-04T09:40:10.066684Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: editdistance in /opt/conda/lib/python3.10/site-packages (0.8.1)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"plt.imshow(\"/kaggle/working/generated/1000.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random, math\nimport numpy as np\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import *\n# import tf.keras.backend as K\nfrom keras.activations import elu\nimport cv2, itertools, sys, editdistance, math\nfrom tensorflow.keras.backend import ctc_batch_cost as ctcLoss\n\nseed = 13\nrandom.seed(seed)\nnp.random.seed(seed)\n\ntf.keras.backend.clear_session()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T09:40:10.069905Z","iopub.execute_input":"2024-11-04T09:40:10.070231Z","iopub.status.idle":"2024-11-04T09:40:13.921945Z","shell.execute_reply.started":"2024-11-04T09:40:10.070195Z","shell.execute_reply":"2024-11-04T09:40:13.921097Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\n\n_, _, files = next(os.walk(\"/kaggle/working/original\"))\nfile_count = len(files)\nprint(file_count)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### data_utils.py","metadata":{}},{"cell_type":"code","source":"def truncateLabel(text, maxStringLen = 32):\n\tcost = 0\n\tfor i in range(len(text)):\n\t\tif i!=0 and text[i] == text[i-1]:\n\t\t\tcost+=2\n\t\telse:\n\t\t\tcost+=1\n\t\tif cost > maxStringLen:\n\t\t\treturn text[:i]\n\treturn text\n\ndef textToLabels(text, unicodes):\n\tret = []\n\tfor c in text:\n\t\tret.append(unicodes.index(c))\n\treturn ret\n\ndef labelsToText(labels, unicodes):\n\tret = []\n\tfor c in labels:\n\t\tif c == len(unicodes):\n\t\t\tret.append(\"\")\n\t\telse:\n\t\t\tret.append(unicodes[c])\n\treturn \"\".join(ret)\n\ndef preprocess(img, dataAugmentation = False):\n\t(wt, ht) = (128, 32)\n\tif img is None:\n\t\timg = (np.zeros((wt, ht, 1))).astype('uint8')\n\timg = cv2.threshold(img, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1] * 255\n\n\tif dataAugmentation:\n\t\tstretch = (random.random() - 0.5) \t\t\t\t\t\t# -0.5 .. +0.5\n\t\twStretched = max(int(img.shape[1] * (1 + stretch)), 1)  # random width, but at least 1\n\t\timg = cv2.resize(img, (wStretched, img.shape[0])) \t\t# stretch horizontally by factor 0.5 .. 1.5\n\timg = closeFit(img)                                         # to avoid lot of white space around text\n\n\th = img.shape[0]\n\tw = img.shape[1]\n\tfx = w / wt\n\tfy = h / ht\n\tf = max(fx, fy)\n\tnewSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1)) \t#scale according to f (result at least 1 and at most wt or ht)\n\timg = cv2.resize(img, newSize, interpolation = cv2.INTER_AREA)   \t\t#INTER_AREA important, Linear loses all info\n\timg = cv2.threshold(img, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1] * 255\n\n\ttarget = np.ones([ht, wt]) * 255\n\ttarget[0:newSize[1], 0:newSize[0]] = img\n\timg = cv2.transpose(target)\n\t(m, s) = cv2.meanStdDev(img)\n\tm = m[0][0]\n\ts = s[0][0]\n\timg = img - m\n\timg = img / s if s>1e-3 else img\n\treturn np.reshape(img, (img.shape[0], img.shape[1], 1))\n\ndef closeFit(img):\n\ti = 2\n\tcol = 255 - np.sum(img, axis=0)/img.shape[0]\n\twhile i<img.shape[1] and col[i]<=5:\n\t\ti+=1\n\tw1 = max(0,i - 15)\n\ti = img.shape[1]-1\n\twhile i>=0 and col[i]<=5:\n\t\ti-=1\n\tw2 = i + 15\n\n\trow = 255 - np.sum(img, axis=1)/img.shape[1]\n\ti = 2\n\twhile i<img.shape[0] and row[i]<=4:\n\t\ti+=1\n\th1 = max(0,i - 20)\n\ti = img.shape[0] - 1\n\twhile i>=0 and row[i]<=5:\n\t\ti-=1\n\th2 = i + 20\n\tfinal = img[h1:h2,w1:w2]\n\tif final.shape[0]*final.shape[1] == 0:\n\t\treturn img\n\treturn final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T09:40:13.923169Z","iopub.execute_input":"2024-11-04T09:40:13.923720Z","iopub.status.idle":"2024-11-04T09:40:13.942996Z","shell.execute_reply.started":"2024-11-04T09:40:13.923684Z","shell.execute_reply":"2024-11-04T09:40:13.942011Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## model_utils.py","metadata":{}},{"cell_type":"code","source":"def predictImage(imgPath, weightPath):\n\timg = cv2.imread(imgPath, 0)\n\timg = preprocess(img, False)\n\timg = np.reshape(img, (1, img2.shape[0], img2.shape[1], 1))\n\tunicodes = list(np.load('/kaggle/input/hindi-htr-unicodes-weights/unicodes (1).npy', allow_pickle = True))\n\tmodel = CRNN(False, len(unicodes + 1))\n\tmodel.load_weights(weightPath)\n\tout = model.predict(img2)\n\tpred = decode(out)\n\tprint('Recognized Word: '+ str(pred))\n\ndef ctcLambdaFunc(yPred, labels, inputLength, labelLength):\n\tyPred = yPred[:,2:,:]\n\tloss = ctcLoss(labels, yPred, inputLength, labelLength)\n\treturn loss\n\ndef decode(yPred, unicodes):  #Best Path Decoder\n\ttexts = []\n\tfor y in yPred:\n\t\tlabel = list(np.argmax(y[2:],1))\n\t\tlabel = [k for k, g in itertools.groupby(label)]\n\t\ttext = labelsToText(label, unicodes)\n\t\ttexts.append(text)\n\treturn texts\n\ndef test(model, loader):\n\tvalidation = loader.valSet\n\ttrueText = []\n\tfor (i, path) in validation:\n\t\ttrueText.append(i)\n\n\t# Wrap the output of loader.nextVal in a tuple to match Keras input expectations\n\n\tbatch_size = 500  # Set this to your desired batch size\n\n\t# Define output_signature for the validation data generator\n\toutput_signature = (\n\t\ttf.TensorSpec(shape=(batch_size, 128, 32, 1), dtype=tf.float32),\n\t)\n\n\tvalidation_data = tf.data.Dataset.from_generator(\n\t\tlambda: loader.nextVal(batch_size),\n\t\toutput_signature=output_signature\n\t)\n\n\toutputs = model.predict(validation_data, steps=math.ceil(loader.valLength / batch_size))\n\tunicodes = list(np.load('/kaggle/input/hindi-htr-unicodes-weights/unicodes (1).npy', allow_pickle = True))\n\tpredText = decode(outputs, unicodes)\n\n\t# print(predText)\n\n\twordOK = 0\n\twordTot = 0\n\tcharDist = 0\n\tcharTot = 0\n\tfor i in range(len(trueText)):\n\t\t#print(predText[i], trueText[i])\n\t\twordOK += 1 if predText[i] == trueText[i] else 0\n\t\twordTot += 1\n\t\tdist = editdistance.eval(predText[i], trueText[i])\n\t\tcharDist += dist\n\t\tcharTot += len(trueText[i])\n\n\tCAR = 100 - 100 * charDist/charTot\n\tWAR = 100 * wordOK/wordTot\n\tprint('Character Accuracy Rate (CAR):' + str(CAR))\n\tprint('Word Accuracy Rate (WAR):' + str(WAR))\n\treturn (CAR, WAR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T09:41:50.961094Z","iopub.execute_input":"2024-11-04T09:41:50.961479Z","iopub.status.idle":"2024-11-04T09:41:50.976186Z","shell.execute_reply.started":"2024-11-04T09:41:50.961443Z","shell.execute_reply":"2024-11-04T09:41:50.975118Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"#### CRNN.py","metadata":{}},{"cell_type":"code","source":"def CRNN(train, outClasses):\n\tinputShape = (128, 32, 1)\n\tkernels = [5, 5, 3, 3, 3]\n\tfilters = [32, 64, 128, 128, 256]\n\tstrides = [(2,2), (2,2), (1,2), (1,2), (1,2)]\n\trnnUnits = 256\n\tmaxStringLen = 32\n\n\tinputs = Input(name = 'inputX', shape = inputShape, dtype = 'float32')\n\tlabels = Input(name='label', shape=[maxStringLen], dtype='float32')\n\tinputLength = Input(name='inputLen', shape=[1], dtype='int64')\n\tlabelLength = Input(name='labelLen', shape=[1], dtype='int64')\n\n\tinner = inputs\n\tfor i in range(len(kernels)):\n\t\tinner = Conv2D(filters[i], (kernels[i], kernels[i]), padding = 'same',\\\n\t\t\t\t\t   name = 'conv' + str(i+1), kernel_initializer = 'glorot_normal') (inner)\n\t\tinner = BatchNormalization() (inner)\n\t\tinner = Activation(elu) (inner)\n\t\tinner = MaxPooling2D(pool_size = strides[i], name = 'max' + str(i+1)) (inner)\n\tinner = Reshape(target_shape = (maxStringLen,rnnUnits), name = 'reshape')(inner)\n\n\tLSF = LSTM(rnnUnits, return_sequences=True, kernel_initializer='glorot_normal', name='LSTM1F') (inner)\n\tLSB = LSTM(rnnUnits, return_sequences=True, go_backwards = True, kernel_initializer='glorot_normal', name='LSTM1B') (inner)\n\tLSB = Lambda(lambda inputTensor: tf.keras.backend.reverse(inputTensor, axes=[1]), output_shape=(maxStringLen, rnnUnits)) (LSB)\n\tLS1 = Average()([LSF, LSB])\n\tLS1 = BatchNormalization() (LS1)\n\n\tLSF = LSTM(rnnUnits, return_sequences=True, kernel_initializer='glorot_normal', name='LSTM2F') (LS1)\n\tLSB = LSTM(rnnUnits, return_sequences=True, go_backwards = True, kernel_initializer='glorot_normal', name='LSTM2B') (LS1)\n\tLSB = Lambda(lambda inputTensor: tf.keras.backend.reverse(inputTensor, axes=[1]), output_shape=(maxStringLen, rnnUnits)) (LSB)\n\tLS2 = Concatenate()([LSF, LSB])\n\tLS2 = BatchNormalization() (LS2)\n\n\tyPred = Dense(outClasses, kernel_initializer='glorot_normal', name='dense2') (LS2)\n\tyPred = Activation('softmax', name='softmax') (yPred)\n\tlossOut = Lambda(ctcLambdaFunc, output_shape=(1,), name='ctc') ([yPred, labels, inputLength, labelLength])\n\n\t# if train:\n\t# \treturn Model(inputs=[inputs, labels, inputLength, labelLength], outputs=[lossOut, yPred])\n\treturn Model(inputs=[inputs], outputs=yPred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T09:40:15.424387Z","iopub.execute_input":"2024-11-04T09:40:15.424786Z","iopub.status.idle":"2024-11-04T09:40:15.439714Z","shell.execute_reply.started":"2024-11-04T09:40:15.424747Z","shell.execute_reply":"2024-11-04T09:40:15.438573Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"#### dataloader.py","metadata":{}},{"cell_type":"code","source":"class DataLoader():\n\tdef __init__(self, trainFile, valFile, unicodes):\n\t\tself.unicodes = unicodes\n\t\tself.trainFile = trainFile\n\t\tself.valFile = valFile\n\t\tself.maxStringLen = 32\n\t\tself.trainSet = []\n\t\tself.valSet = []\n\t\tself.trainIndex = 0\n\t\tself.valIndex = 0\n\n\t\tif self.trainFile != \"\":\n\t\t\tself.trainSet = self.importSets(True)\n\t\tif self.valFile != \"\":\n\t\t\tself.valSet = self.importSets(False)\n\t\tself.valLength = len(self.valSet)\n\t\tself.trainLength = len(self.trainSet)\n\n\tdef importSets(self, train):\n\t\tset = []\n\t\tif train:\n\t\t\tfile = open(self.trainFile, 'r', encoding='utf-8')\n\t\telse:\n\t\t\tfile = open(self.valFile, 'r', encoding='utf-8')\n\t\tfor line in file:\n\t\t\tinUnicodes = True\n\t\t\tif not line or line[0] =='#':\n\t\t\t\t#Ignoring Erroneous Lines manually skipped with # in file\n\t\t\t\tcontinue\n\t\t\tlineSplit = line.strip().split(' ')\n\t\t\tif len(lineSplit) >= 2:\n\t\t\t\tfileName = lineSplit[0]\n\t\t\t\ttext = truncateLabel(' '.join(lineSplit[1:]))\n\n\t\t\t\tfor ch in text:\n\t\t\t\t\tif not ch in self.unicodes:\n\t\t\t\t\t\tprint('Char '+ str(ch)+ ' Not in Unicodes, and Word Omitted')\n\t\t\t\t\t\t#print(ch,('0'+hex(ord(ch))[2:]))\n\t\t\t\t\t\tinUnicodes = False\n\n\t\t\t\tif inUnicodes:\n\t\t\t\t\tif train:\n\t\t\t\t\t\tset.append((text, fileName))\n\t\t\t\t\telse:\n\t\t\t\t\t\tset.append((text, fileName))\n\t\t\telse:\n\t\t\t\tprint(line + 'Check this Line')\n\t\tfile.close()\n\t\t# random.shuffle(set)\n\t\treturn set\n\n\tdef nextTrain(self, batchSize):\n\t\twhile True:\n\t\t\tif self.trainIndex + batchSize >= self.trainLength:\n\t\t\t\tself.trainIndex = 0\n\t\t\t\trandom.shuffle(self.trainSet)\n\t\t\tret = self.getBatch(self.trainIndex, batchSize, True)\n\t\t\tself.trainIndex += batchSize\n\t\t\tyield ret\n\n\tdef nextVal(self, batchSize):\n\t\twhile True:\n\t\t\tif self.valIndex >= self.valLength:\n\t\t\t\tself.valIndex = 0\n\t\t\tret = self.getBatch(self.valIndex, batchSize, False)\n\t\t\tself.valIndex += batchSize\n\t\t\tyield (ret,)\n\n\tdef getBatch(self, index, batchSize, train):\n\t\tif train:\n\t\t\tbatch = self.trainSet[index:index + batchSize]\n\t\t\tsize = self.trainLength\n\t\telse:\n\t\t\tbatch = self.valSet[index:index + batchSize]\n\t\t\tsize = self.valLength\n\n\t\timgs = []\n\t\tlabels = np.ones([batchSize, self.maxStringLen]) * len(self.unicodes)\n\t\tinputLength = np.zeros([batchSize, 1])\n\t\tlabelLength = np.zeros([batchSize, 1])\n\n\t\tfor i in range(min(batchSize, size-index)):\n\t\t\timg = cv2.imread(batch[i][1], 0)\n\t\t\tif img is None:\n\t\t\t\timg = np.zeros((128,32,1))\n\t\t\t\tprint(batch[i][1] + 'is not available')\n\n\t\t\timg = img.astype('uint8')\n\t\t\timgs.append(preprocess(img.astype('uint8'), train))\n\t\t\tlabels[i, 0:len(batch[i][0])] = textToLabels(batch[i][0], self.unicodes)\n\t\t\tlabelLength[i] = len(batch[i][0])\n\t\t\tinputLength[i] = self.maxStringLen - 2\n\n\t\tinputs = {\n\t\t\t\t'inputX' : np.asarray(imgs),\n\t\t\t\t'label' : labels,\n\t\t\t\t'inputLen' : inputLength,\n\t\t\t\t'labelLen' : labelLength,\n\t\t\t\t\t}\n\t\toutputs = {'ctc' : np.zeros([batchSize])}\n\t\tif train:\n\t\t\treturn (inputs, outputs)\n\t\telse:\n\t\t\treturn imgs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T09:40:17.500576Z","iopub.execute_input":"2024-11-04T09:40:17.501325Z","iopub.status.idle":"2024-11-04T09:40:17.524443Z","shell.execute_reply.started":"2024-11-04T09:40:17.501262Z","shell.execute_reply":"2024-11-04T09:40:17.523433Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## generate path - label pairs","metadata":{}},{"cell_type":"code","source":"annotations = None\nwith open(\"/kaggle/input/hindi-htr-calam-dataset-annotations/annotations.txt\", \"r\", encoding=\"utf-8\") as f:\n    annotations = f.readlines()\n    annotations = [x.replace(\"\\n\", \"\") for x in annotations]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T09:40:21.876262Z","iopub.execute_input":"2024-11-04T09:40:21.877143Z","iopub.status.idle":"2024-11-04T09:40:21.996927Z","shell.execute_reply.started":"2024-11-04T09:40:21.877105Z","shell.execute_reply":"2024-11-04T09:40:21.996057Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"start_range, end_range = 20000, 100000\nannotation_subset = []\nfor i in range(start_range - 1, end_range - 1):\n    annotation_subset.append(annotations[i])\n\nog_pairs, gen_pairs, tgt_pairs = [], [], []\nfor i in range(1, end_range - start_range + 1):\n    og_pairs.append(f\"{og_dir}/{i}.png {annotation_subset[i-1]}\\n\")\n    gen_pairs.append(f\"{gen_dir}/{i}.png {annotation_subset[i-1]}\\n\")\n    tgt_pairs.append(f\"{tgt_dir}/{i}.png {annotation_subset[i-1]}\\n\")\n\nwith open(\"/kaggle/working/og_pairs.txt\", \"w+\") as file: file.writelines(og_pairs)\nwith open(\"/kaggle/working/gen_pairs.txt\", \"w+\") as file: file.writelines(gen_pairs)\nwith open(\"/kaggle/working/tgt_pairs.txt\", \"w+\") as file: file.writelines(tgt_pairs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T09:40:55.054826Z","iopub.execute_input":"2024-11-04T09:40:55.055475Z","iopub.status.idle":"2024-11-04T09:40:55.395357Z","shell.execute_reply.started":"2024-11-04T09:40:55.055422Z","shell.execute_reply":"2024-11-04T09:40:55.394329Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"unicodes = list(np.load('/kaggle/input/hindi-htr-unicodes-weights/unicodes (1).npy', allow_pickle = True))\n\nog_loader = DataLoader('', \"/kaggle/working/og_pairs.txt\", unicodes)\ngen_loader = DataLoader('', \"/kaggle/working/gen_pairs.txt\", unicodes)\ntgt_loader = DataLoader('', \"/kaggle/working/tgt_pairs.txt\", unicodes)\ntestModel = CRNN(False, len(unicodes) + 1)\ntestModel.load_weights('/kaggle/input/hindi-htr-unicodes-weights/crnn_weights_exp2.h5')\n\nprint(\"\\nHTR Metrics on Original Images\")\nCAR, WAR = test(testModel, og_loader)\nprint(\"\\nHTR Metrics on Generated Images\")\nCAR, WAR = test(testModel, gen_loader)\nprint(\"\\nHTR Metrics on Target Images\")\nCAR, WAR = test(testModel, tgt_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T09:41:56.518944Z","iopub.execute_input":"2024-11-04T09:41:56.519828Z","iopub.status.idle":"2024-11-04T09:49:33.693387Z","shell.execute_reply.started":"2024-11-04T09:41:56.519779Z","shell.execute_reply":"2024-11-04T09:49:33.692224Z"}},"outputs":[{"name":"stdout","text":"\nHTR Metrics on Original Images\n\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 907ms/step\nCharacter Accuracy Rate (CAR):82.29901716934125\nWord Accuracy Rate (WAR):38.8425\n\nHTR Metrics on Generated Images\n\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 913ms/step\nCharacter Accuracy Rate (CAR):23.61925701195733\nWord Accuracy Rate (WAR):0.57125\n\nHTR Metrics on Target Images\n\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 914ms/step\nCharacter Accuracy Rate (CAR):79.89813444370066\nWord Accuracy Rate (WAR):38.37625\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"tf.keras.backend.clear_session()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T09:50:06.828148Z","iopub.execute_input":"2024-11-04T09:50:06.828537Z","iopub.status.idle":"2024-11-04T09:50:07.078681Z","shell.execute_reply.started":"2024-11-04T09:50:06.828499Z","shell.execute_reply":"2024-11-04T09:50:07.077573Z"}},"outputs":[],"execution_count":16}]}